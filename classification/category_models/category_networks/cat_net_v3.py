import tensorflow_datasets as tfds
import pandas as pd
import tensorflow as tf
import keras

from keras import layers, losses
from keras.preprocessing.text import one_hot
from keras.models import Sequential
from keras.layers import Flatten, TextVectorization, Dense, Embedding, Bidirectional, LSTM
from keras.utils import to_categorical
import keras_nlp
from typing import List
from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab

from keras_preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score

from .base_category_network import BaseCategoryNetwork
from ...preprocessing import Category
from ...preprocessing.categorize_prepare import create_feature_list, get_df_from_feature_list_str_features, \
    encode_class_values
from ...preprocessing.keras_text_preprocesses import load_glove_embedding, prepare_embeddings_matrix

tfds.disable_progress_bar()
"""
Because of my previous problems, I'll keep this so I can always reset to this step or at least refer back 
to the minimum working version.
"""
class CategoryNetworkV3(BaseCategoryNetwork):
    BATCH_SIZE = 64
    EPOCHS = 3
    MAX_SEQUENCE_LENGTH = 512
    VOCAB_SIZE = 15000

    EMBED_DIM = 128
    INTERMEDIATE_DIM = 512

    def __init__(self, name: str, **kwargs):
        super().__init__(name=name, version='v3', description='HTML classification using keras.TextVectorization.')
        keras.utils.set_random_seed(42)

    """
    Method loads the saved trained model and predicts the categories for the input web_ids.
    Preparation of the input generated by the web_ids is handled through the method 
    get_df_from_feature_list_inkl_keyword_result which returns a dataframe with the web_ids as index.
    """
    def predict(self, web_ids: List[str]) -> List[Category]:
        self.load()
        feature_dict = get_df_from_feature_list_str_features(create_feature_list(web_ids), web_ids)
        X = pd.get_dummies(feature_dict['html'])
        #y_test = feature_dict['true_category'].apply(lambda x: int(x))
        y_test = to_categorical(feature_dict['true_category'], num_classes=8)
        # loss, accuracy = self.model.predict(feature_dict)
        # print("Accuracy: {:2.2%}".format(binary_accuracy))
        y_hat = self.model.predict(X)
        print(y_hat)
        y_hat
        # y_hat = model.predict(X_test)
        #y_hat = [0 if val < 0.5 else 1 for val in y_hat] #for 0-1 classes
        #accuracy_score(y_test, y_hat)
        # todo reformatting the categorys back from int-matrix (encoded) -> str
        return y_hat


    """
    Method is base method of a classificator method and forwards into the predict method.
    """
    def classification(self, web_ids: List[str]) -> List[Category]:
        return self.predict(web_ids)



    """
    Method is base method of a classificator method and forwards into the predict method.
    Preparation of the input generated by the web_ids is handled through the method 
    get_df_from_feature_list_inkl_keyword_result which returns a dataframe with the web_ids as index.
    The keras model built here, has 8 output classes (9 will be none for keyword based), uses softmax as the last layer.
    Optimizer is adam, SparseCategoricalCrossentropy, and the accuracy metrics.
    Training length here is only used for testing: epochs=24, batch_size=32
    """
    def train(self, web_ids: List[str]) -> None:
        feature_dict = get_df_from_feature_list_str_features(create_feature_list(web_ids))
        X_train = feature_dict['html']
        # encoder, y_train = encode_class_values(feature_dict['true_category'])
        y_train = feature_dict['true_category']
        #train_ds = X_train.assign(Lower_HTML = lambda x: (tf.strings.lower(x['html'])))
        train_ds = X_train.apply(lambda x: tf.strings.lower(x))

        reserved_tokens = ["[PAD]", "[UNK]"]
        train_sentences = [element for element in train_ds]

        def train_word_piece(ds, reserved_tokens):
            bert_vocab_args = dict(
                # The target vocabulary size
                vocab_size=self.VOCAB_SIZE,
                # Reserved tokens that must be included in the vocabulary
                reserved_tokens=reserved_tokens,
                # Arguments for `text.BertTokenizer`
                bert_tokenizer_params={"lower_case": True},
            )

            vocab = bert_vocab.bert_vocab_from_dataset(
                ds[0], **bert_vocab_args
            )
            return vocab
        vocab = train_word_piece(train_ds, reserved_tokens)

        tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
            vocabulary=vocab,
            lowercase=False,
            sequence_length=self.MAX_SEQUENCE_LENGTH,
        )
        input_sentence_ex = train_ds.take(1).get_single_element()[0][0]
        input_tokens_ex = tokenizer(input_sentence_ex)

        print("Sentence: ", input_sentence_ex)
        print("Tokens: ", input_tokens_ex)
        print("Recovered text after detokenizing: ", tokenizer.detokenize(input_tokens_ex))

#tf.convert_to_tensor()
        def format_dataset(sentence, label):
            sentence = tokenizer(sentence)
            return ({"input_ids": sentence}, label)

        def make_dataset(dataset):
            dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)
            return dataset.shuffle(512).prefetch(16).cache()

        train_ds = self.make_dataset(tf.data.Dataset.from_tensor_slices((train_ds, y_train)))
        #train_ds = self.make_dataset((train_ds.values, y_train.values))

        input_ids = keras.Input(shape=(None,), dtype="int64", name="input_ids")

        x = keras_nlp.layers.TokenAndPositionEmbedding(
            vocabulary_size=self.VOCAB_SIZE,
            sequence_length=self.MAX_SEQUENCE_LENGTH,
            embedding_dim=self.EMBED_DIM,
            mask_zero=True,
        )(input_ids)

        x = keras_nlp.layers.FNetEncoder(intermediate_dim=self.INTERMEDIATE_DIM)(inputs=x)
        x = keras_nlp.layers.FNetEncoder(intermediate_dim=self.INTERMEDIATE_DIM)(inputs=x)
        x = keras_nlp.layers.FNetEncoder(intermediate_dim=self.INTERMEDIATE_DIM)(inputs=x)

        x = keras.layers.GlobalAveragePooling1D()(x)
        x = keras.layers.Dropout(0.1)(x)
        outputs = keras.layers.Dense(1, activation="softmax")(x)

        model = keras.Model(input_ids, outputs, name="fnet_classifier")
        model.summary()

        model.compile(
            loss=losses.SparseCategoricalCrossentropy(from_logits=True),
            optimizer='adam',
            metrics='accuracy')
        model.fit(X_train, y_train, epochs=24, batch_size=32)
        self.model = model
        self.save()
        # (number of inputs + 1 output)/2 = #hiddenLayers
        pass
